---
title: "space_time"
author: "lbanin"
date: "11/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
setwd("N:/Lindsay Banin/Stats Methods/Bayesian_Course_Sept2019/Spatial_SpatioTemp")
library(spTimer)
library(sp)
library(geoR)

```

# Case study: New York Ozone
- Response: Maximum 8hr mean average ground level ozone (o8hrmax)
- Temporal Structure: Daily average for two months, July & August in 2006 
- Spatial structure: Observed at 28 sites (s.index) across NY state
- Covariates: maximum temperature, windspeed and relative humidity (cMAXTMP, WDSP, RH)

First let's explore the data. Note, the data must be indexed first by the location index and then for each location, orded by time. The length of the time series must be the same for each location (with missing values denoted as NA). 
Here, we also set aside 8 sites for spatial validation purposes and two days for temporal validation and plot up the data.
```{r}
data("NYdata")
str(NYdata)

# DataFit will be for estimating the model and DataValPred will be for model validation
s.val <- c(8,11,12,14,18,21,24,28)
DataFit <- spT.subset(data = NYdata, var.name="s.index", s=s.val, reverse=TRUE)
# NB reverse = TRUE!
DataFit <- subset(DataFit, with(DataFit, !(Day %in% c(30,31) & Month==8)))
DataValPred <- spT.subset(data = NYdata, var.name="s.index", s=s.val)
DataValPred <- subset(DataValPred, with(DataValPred, !(Day %in% c(30,31) & Month==8)))

boundary <- read.csv(file="NYboundary.csv")
plot(boundary, type="l", main="Map of New York area", xlab="Longitude", ylab="Latitude")
points(DataFit$Longitude, DataFit$Latitude, pch=19, col="red")
points(DataValPred$Longitude, DataValPred$Latitude, pch=19, col="blue")
```

# Model estimation

Assuming spatial and temporal independence, what would a regression model look like?

```{r}
lm1 <- lm(formula=sqrt(o8hrmax)~cMAXTMP+WDSP+RH, data=DataFit)
summary(lm1)
round(cbind(lm1$coefficients, confint(lm1)),3)
```


We use the 'spT.Gibbs' function to estimate the spatio-temporal models.
```{r}
args(spT.Gibbs)
```




First we fit a Gaussian process (GP) model - this includes the temporal data but does not model the temporal correlation.
The linear (covariate) part is specified with the 'formula' argument.
The square root transformation is used on the fly to stabilise the variance.
Here we opt for default initial values and the values of the hyper-parameters of the priors are assumed by default; the hyper-parameters of the phi (spatial decay) Gamma prior are specified. We select the 'exponential' spatial covariance function.
We also supply arguments to control the running of the MCMC algorithm (nItr, nBurn). 

```{r}
set.seed(11)
# Set up the time structure
sptime <- spT.time(t.series=60, segment=1)
# Estimate model
post.gp1 <- spT.Gibbs(formula = o8hrmax ~ 1,
                     data = DataFit, model="GP",
                     coords= ~ Longitude + Latitude, scale.transform = "SQRT",
                     cov.fnc = "exponential",  spatial.decay=spT.decay(distribution=Gamm(2,1), tuning = 0.1), time.data=sptime, nItr = 10000, nBurn = 1000)
# Look at the summary stats of the posteriors
summary(post.gp1)
```
```{r}
# We can also plot the trace plots and posterior distributions for the parameters
plot(post.gp1)
```

Now we fit an Autoregressive Model (AR)

```{r}
set.seed(11)
post.ar1 <- spT.Gibbs(formula = o8hrmax ~ 1,
                     data = DataFit, model="AR",
                     coords= ~ Longitude + Latitude, scale.transform = "SQRT",
                     cov.fnc = "exponential",  spatial.decay=spT.decay(distribution=Gamm(2,1), tuning = 0.1), time.data=sptime, nItr = 10000, nBurn = 1000)
summary(post.ar1)


```

Examine some output:
```{r}
plot(post.ar1)

```

```{r}
# We can examine the temporal autocorrelation
library(coda)
autocorr.diag(as.mcmc(post.ar1))
autocorr.plot(as.mcmc(post.ar1))
```

Now let's fit the covariates.

```{r}
set.seed(11)
post.ar2 <- spT.Gibbs(formula = o8hrmax ~ cMAXTMP+WDSP+RH,
                     data = DataFit, model="AR",
                     coords= ~ Longitude + Latitude, scale.transform = "SQRT",
                     cov.fnc = "exponential",  spatial.decay=spT.decay(distribution=Gamm(2,1), tuning = 0.1), time.data=sptime, nItr = 10000, nBurn = 1000)
summary(post.ar2)

```

Notice how the sig2eta - spatial variance - changes across models; much less so for the nugget term sig2eps.

We can also look at the autocorrelation.
```{r}
autocorr.diag(as.mcmc(post.ar2))
autocorr.plot(as.mcmc(post.ar2))
```

```{r}
# How do the model residuals look?
plot(post.ar2, residuals = TRUE)

```

# Model prediction and validation

Here we use the 'predict' function.

We can use the function 'spT.validation' to extract various validation criteria - the function requires the data values and predicted values. In this example we are comparing the data for 8 locations across 60 days.
The function returns the following measures:
MSE Mean Squared Error.
RMSE Root Mean Squared Error.
MAE Mean Absolute Error.
MAPE Mean Absolute Percentage Error.
BIAS Bias.
rBIAS Relative Bias.
rMSEP Relative Mean Separation

```{r}
set.seed(11)
pred.gp1 <- predict(post.gp1, newdata=DataValPred, newcoords=~Longitude+Latitude)
print(pred.gp1)

spT.validation(DataValPred$o8hrmax, c(pred.gp1$Median))
```

```{r}
set.seed(11)
pred.ar2 <- predict(post.ar2, newdata=DataValPred, newcoords=~Longitude+Latitude)
print(pred.ar2)
spT.validation(DataValPred$o8hrmax, c(pred.ar2$Median))
```

How do the two models perform?
Let's also compare visually.

```{r}

par(mfrow= c(1,2))
plot(c(pred.gp1$Mean) ~ DataValPred$o8hrmax)
plot(c(pred.ar2$Mean) ~ DataValPred$o8hrmax)
```

We can also map across the whole domain:
```{r}
# Create the prediction grid
long <- seq(-80, -73, length.out=20)
lat <- seq(40, 45, length.out=20)
predgrid <- polygrid(x=long, y=lat, border=boundary)
plot(predgrid, xlim=c(-80, -73), ylim=c(40, 45))
lines(boundary)

pred.gp1 <- predict.spT(object=post.gp1, newcoords=predgrid, type="spatial")
predgrid.gp1 <- data.frame(t(pred.gp1$Mean)[ ,c(1, 5, 10)])
colnames(predgrid.gp1) <- c("day1", "day5", "day10")
sp.class.gp <- SpatialPixelsDataFrame(points=predgrid, data=predgrid.gp1)
boundary.line <- Line(boundary)
lineplot = list("sp.lines", boundary.line, col="black", lwd=2)
spplot(sp.class.gp, "day1", sp.layout=list(lineplot), xlim=c(-80, -73), ylim=c(40, 45.5))

pred.ar2 <- predict.spT(object=post.ar2, newcoords=predgrid, type="spatial")
predgrid.ar2 <- data.frame(t(preds.ar$Mean)[ ,c(1, 5, 10)])
colnames(predgrid.ar2) <- c("day1", "day5", "day10")
sp.class.ar <- SpatialPixelsDataFrame(points=predgrid, data=predgrid.ar2)
spplot(sp.class.ar, "day1", sp.layout=list(lineplot), xlim=c(-80, -73), ylim=c(40, 45.5))

```

# Open-ended exercises:
- As with the spatial analyses, you can examine the impact of spatial covariance models on outputs/prediction
- Compare the full set of 4 models (GP with and without covariates; AR with and without covariates). Look at what happens to the posteriors of the priors (means and CIs) and examine the validation diagnostics. Which seems to be performing best?
- Play with plotting - how different are the maps for the same time point using different models? Is the difference important?