---
title: "MCMC"
author: "MvO"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    toc: yes
  word_document: default
  pdf_document:
    number_sections: yes
    toc: yes
subtitle: An introduction with code examples in R
editor_options: 
  chunk_output_type: inline
---

# Bayesian regression

## Bayesian quadratic regression

We begin by getting our data, defining our likelihood function, model and prior, and the settings of the MCMC that we are going to do.

```{r}
require(mvtnorm)
chainLength <- 20000
data <- matrix( c(10,  6.09, 1.83,
                  20,  8.81, 2.64,
                  30, 10.66, 3.27),
                nrow=3, ncol=3, byrow=T) ; nd <- nrow(data)
Li   <- function( model, theta, data, i ) {
          dnorm( (model(data[i,1],theta)-data[i,2])/data[i,3] ) }
L    <- function( model, theta, data    ) {
          prod( sapply(1:nd,function(i){Li(model,theta,data,i)}) ) }
logL <- function( model, theta, data  ) { log(L(model,theta,data)) }
# model       <- function (x,theta) { theta[1]*x + theta[2] }
model       <- function (x,theta) { theta[1]*x + theta[2] + theta[3]*x^2 }
# np           <- 2
np           <- 3
# prior        <- matrix( c(0, 1,
#                           0,10), nrow=np, byrow=T )
prior        <- matrix( c( 0  , 1,
                           0  ,10,
                          -0.1, 0.1), nrow=np, byrow=T )
pMin         <- prior[,1] ; pMax <- prior[,2]
pVector      <- rowMeans(prior)
pChain       <- matrix(, nrow=chainLength, ncol=np)
pChain[1,]   <- pVector
logPrior0    <- sum( log( dunif(pVector,pMin,pMax) ) )

logL0        <- logL(model,pVector,data)
logLChain    <- matrix(, nrow=chainLength, ncol=1)
logLChain[1] <- logL0

vcovProposal <- diag( ( 0.2*(pMax-pMin) )^2 )
```

Now we are ready to run the MCMC itself.

```{r}
for (c in 2:chainLength) {
  cint <- min(1000,round(chainLength/10))
  if (c%%cint == 0) cat("Iteration",c,"|",logPrior0,"|",logL0,"\n")
  candidatepVector <- rmvnorm(n=1, mean=pVector, sigma=vcovProposal)
  logPrior1   <- sum(log(dunif(candidatepVector, pMin, pMax)))
  if (!is.na(logPrior1)) { 
    logL1    <- logL(model,candidatepVector,data)
    logalpha <- logPrior1 + logL1 - logPrior0 - logL0
    if (log(runif(1, min=0, max=1)) < logalpha) {
      pVector   <- candidatepVector
      logPrior0 <- logPrior1
      logL0     <- logL1                       } }
     pChain[c,]    <- pVector
     logLChain[c,] <- logL0   }
```

Let's examine the MCMC-results.

```{r}
nAccepted  <- length(unique(logLChain))
acceptance <- (paste(nAccepted, "out of ", chainLength,
                     "candidates accepted ( = ",
                     round(100*nAccepted/chainLength), "%)"))
print(acceptance)
mp        <- colMeans(pChain)                ; print(mp)
iMAP      <- match(max(logLChain),logLChain)
MAP       <- pChain[iMAP,]                  ; print(MAP)
pCovMatrix<- cov(pChain)                     ; print(pCovMatrix)
pCorMatrix<- cor(pChain)                     ; print(pCorMatrix)
sp        <- sqrt( diag(pCovMatrix) )       ; print(sp)
plot  ( 1:30,model(1:30,mp ), col="black",lwd=3,type="l" )
lines ( 1:30,model(1:30,MAP), col="red"  ,lwd=3,type="p" )
points( data                  , col='blue' ,lwd=3,cex=2    )
```

# EXERCISES

All these exercises ask you to modify the MCMC-code for linear regression. We want to keep the original code, so copy the code-chunks to this section (or to a new Rmd-file), and modify that copy.

1. Make the data more informative by reducing the standard deviation by a factor 10. What effects does that have on the Bayesian calibration of the straight-line model?
2. Same question but **increasing** the uncertainty by a factor 10.
3. If you would make the uncertainty about one of the three data points much higher than for the other points, how would that affect the calibration?
4. Modify the code for linear regression: replace the straight line (2 parameters) by a quadratic curve (3 parameters). Can you get the code to work? How would you decide whether the linear or the quadratic curve is the best one? How would you define 'best' in a Bayesian way?
